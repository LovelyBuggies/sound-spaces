{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Shuo Test config files\n",
    "import time, os, quaternion, habitat_sim\n",
    "os.chdir(\"/home/ubuntu/avlm/sound-spaces\")\n",
    "\n",
    "backend_cfg = habitat_sim.SimulatorConfiguration()\n",
    "backend_cfg.scene_id = \"data/scene_datasets/gibson/Oyens.glb\"\n",
    "backend_cfg.scene_dataset_config_file = \"data/scene_datasets/gibson/gibson_semantic.scene_dataset_config.json\"\n",
    "backend_cfg.load_semantic_mesh = True\n",
    "backend_cfg.enable_physics = False\n",
    "\n",
    "t=time.time()\n",
    "agent_config = habitat_sim.AgentConfiguration()\n",
    "print(\"agent ok\", time.time()-t, flush=True)\n",
    "\n",
    "t=time.time()\n",
    "cfg = habitat_sim.Configuration(backend_cfg, [agent_config])\n",
    "print(\"cfg ok\", time.time()-t, flush=True)\n"
   ],
   "id": "3887322ebb02d9eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%capture\n",
    "import os\n",
    "import quaternion\n",
    "import habitat_sim.sim\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "\n",
    "\n",
    "os.chdir('/home/ubuntu/avlm/sound-spaces')\n",
    "dataset = 'gibson' # or replace with 'mp3d', one example for each dataset\n",
    "\n",
    "backend_cfg = habitat_sim.SimulatorConfiguration()\n",
    "if dataset == 'mp3d':\n",
    "    backend_cfg.scene_id = \"data/scene_datasets/mp3d/UwV83HsGsw3/UwV83HsGsw3.glb\"\n",
    "    # IMPORTANT: missing this file will lead to load the semantic scene incorrectly\n",
    "    backend_cfg.scene_dataset_config_file = \"data/scene_datasets/mp3d/mp3d.scene_dataset_config.json\"\n",
    "else:\n",
    "    backend_cfg.scene_id = \"data/scene_datasets/gibson/Oyens.glb\"\n",
    "    # IMPORTANT: missing this file will lead to load the semantic scene incorrectly\n",
    "    backend_cfg.scene_dataset_config_file = \"data/scene_datasets/gibson/gibson_semantic.scene_dataset_config.json\"\n",
    "backend_cfg.load_semantic_mesh = True\n",
    "backend_cfg.enable_physics = False\n",
    "agent_config = habitat_sim.AgentConfiguration()\n",
    "cfg = habitat_sim.Configuration(backend_cfg, [agent_config])\n",
    "sim = habitat_sim.Simulator(cfg)\n",
    "\n",
    "# set navmesh path for searching for navigable points\n",
    "if dataset == 'mp3d':\n",
    "    sim.pathfinder.load_nav_mesh(os.path.join(f\"data/scene_datasets/mp3d/UwV83HsGsw3/UwV83HsGsw3.navmesh\"))\n",
    "else:\n",
    "    sim.pathfinder.load_nav_mesh(os.path.join(f\"data/scene_datasets/gibson/Oyens.navmesh\"))\n",
    "\n",
    "audio_sensor_spec = habitat_sim.AudioSensorSpec()\n",
    "audio_sensor_spec.uuid = \"audio_sensor\"\n",
    "audio_sensor_spec.enableMaterials = True\n",
    "audio_sensor_spec.channelLayout.type = habitat_sim.sensor.RLRAudioPropagationChannelLayoutType.Binaural\n",
    "audio_sensor_spec.channelLayout.channelCount = 1\n",
    "# audio sensor location set with respect to the agent\n",
    "audio_sensor_spec.position = [0.0, 1.5, 0.0]  # audio sensor has a height of 1.5m\n",
    "audio_sensor_spec.acousticsConfig.sampleRate = 48000\n",
    "# whether indrect (reverberation) is present in the rendered IR\n",
    "audio_sensor_spec.acousticsConfig.indirect = True\n",
    "sim.add_sensor(audio_sensor_spec)\n",
    "\n",
    "audio_sensor = sim.get_agent(0)._sensors[\"audio_sensor\"]\n",
    "audio_sensor.setAudioMaterialsJSON(\"data/mp3d_material_config.json\")"
   ],
   "id": "b32aa0eaf503b728"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# sampled navigable point is on the floor\n",
    "source_pos = sim.pathfinder.get_random_navigable_point()\n",
    "print('Sample source location: ', source_pos)"
   ],
   "id": "3cab77bfb64cb349"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4076415",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_sensor.setAudioSourceTransform(source_pos + np.array([0, 1.5, 0])) # add 1.5m to the height calculation \n",
    "agent = sim.get_agent(0)\n",
    "new_state = sim.get_agent(0).get_state()\n",
    "new_state.position = np.array(source_pos + np.array([2, 0, 0]))\n",
    "new_state.sensor_states = {}\n",
    "agent.set_state(new_state, True)\n",
    "ir = np.array(sim.get_sensor_observations()[\"audio_sensor\"])\n",
    "print(ir.shape)\n",
    "\n",
    "# one a category is not found in the material mapping file, the default acoustic material will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the direct sound is present (source is visibile from the listener)\n",
    "audio_sensor.sourceIsVisible()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3974eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the efficiency of rendering, outdoor would have a very low value, e.g. < 0.05, \n",
    "# while a closed indoor room would have >0.95, and a room with some holes might be in the 0.1-0.8 range.\n",
    "# if the ray efficiency is low for an indoor environment, it indicates a lot of ray leak from holes\n",
    "# you should repair the mesh in this case for more accurate acoustic rendering\n",
    "audio_sensor.getRayEfficiency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9630852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the waveform of IR and show the audio\n",
    "from librosa.display import waveshow, specshow\n",
    "import IPython\n",
    "\n",
    "waveshow(ir[0, :10000], sr=48000)\n",
    "IPython.display.Audio(ir, rate=48000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf1f1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one example for how to use IR data to get the reverberant speech\n",
    "sr, vocal = wavfile.read('res/singing.wav')\n",
    "print(sr, vocal.shape)\n",
    "IPython.display.Audio(vocal, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096cd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import fftconvolve\n",
    "\n",
    "# convolve the vocal with IR\n",
    "convolved_vocal = np.array([fftconvolve(vocal, ir_channel) for ir_channel in ir]) \n",
    "IPython.display.Audio(convolved_vocal, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e4d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyroomacoustics.experimental.rt60 import measure_rt60\n",
    "\n",
    "rt60 = measure_rt60(ir[0], sr, decay_db=30, plot=True)\n",
    "print(f'RT60 of the rendered IR is {rt60:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e30b1c-cd69-44bf-a820-81f43a2b5670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ss)",
   "language": "python",
   "name": "ss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  },
  "vscode": {
   "interpreter": {
    "hash": "39440bdb03f68d519f97c71a0ad1591dc1b27ff6078f991f0526c5f3c0823f0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
